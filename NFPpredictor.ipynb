{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNp1yZ+TwrJxDV6bcAFP8A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/semunyujunior/WhatsAppcode/blob/main/NFPpredictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrpFxs5pQuuc",
        "outputId": "739cc3f2-892a-4861-950b-a7e393744602",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.47.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (24.2)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from shap) (4.14.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy scikit-learn xgboost lightgbm shap matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸš€ PRO NFP MODEL - REGRESSION PIPELINE\n",
        "# Built with Victor - Fund-grade NFP signal\n",
        "# Split version: REGRESSION\n",
        "\n",
        "# ==========================================\n",
        "# STEP 1 â€” IMPORT LIBRARIES\n",
        "# ==========================================\n",
        "\n",
        "# Core\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ML\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "# Explainability\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Misc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ… Libraries loaded ðŸš€\")\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# STEP 2 â€” LOAD DATA\n",
        "# ==========================================\n",
        "# ðŸ‘‰ You can modify this path to your current CSV or DataFrame!\n",
        "\n",
        "# Example: Load your pre-engineered feature dataset\n",
        "# (For demo here I'll create a dummy DataFrame â€” replace with your real one!)\n",
        "\n",
        "# Example columns used in the project:\n",
        "feature_columns = [\n",
        "    'Delta_Prev_Forecast',\n",
        "    'Momentum',\n",
        "    'Month',\n",
        "    'Quarter',\n",
        "    'Lag1_NFP',\n",
        "    'Lag2_NFP',\n",
        "    'Rolling_Vol_3M',\n",
        "    'Rolling_Vol_6M',\n",
        "    'ADP_Pct_Change',\n",
        "    'Jobless_4Wk_Avg',\n",
        "    'ISM_PMI_1M_Change',\n",
        "    'ISM_PMI_3M_Avg',\n",
        "    'Yield_10Y_Change'\n",
        "]\n",
        "\n",
        "# Load your dataset:\n",
        "df = pd.read_csv('YOUR_FEATURE_DATASET.csv')  # Replace with your feature dataset path!\n",
        "\n",
        "# Target: Classification â†’ NFP Higher (1) / Lower (0)\n",
        "target_class = 'Target_Classification'  # Your target column\n",
        "\n",
        "# Features & Target\n",
        "X = df[feature_columns]\n",
        "y = df[target_class]\n",
        "\n",
        "print(\"âœ… Data loaded â€” Shape:\", X.shape)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# STEP 3 â€” TRAIN/TEST SPLIT\n",
        "# ==========================================\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"âœ… Split done â€” Train size: {X_train.shape[0]}, Test size: {X_test.shape[0]}\")\n",
        "\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# STEP 4 â€” XGBOOST CLASSIFIER + TUNING\n",
        "# ==========================================\n",
        "\n",
        "xgb_clf = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "xgb_param_grid = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'subsample': [0.8, 1],\n",
        "    'colsample_bytree': [0.8, 1]\n",
        "}\n",
        "\n",
        "xgb_search = RandomizedSearchCV(\n",
        "    estimator=xgb_clf,\n",
        "    param_distributions=xgb_param_grid,\n",
        "    n_iter=20,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_search.fit(X_train, y_train)\n",
        "\n",
        "best_xgb = xgb_search.best_estimator_\n",
        "print(\"âœ… XGBoost Classifier tuned â€” Best params:\", xgb_search.best_params_)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# STEP 5 â€” LIGHTGBM CLASSIFIER + TUNING\n",
        "# ==========================================\n",
        "\n",
        "lgbm_clf = LGBMClassifier(random_state=42)\n",
        "\n",
        "lgbm_param_grid = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'subsample': [0.8, 1],\n",
        "    'colsample_bytree': [0.8, 1]\n",
        "}\n",
        "\n",
        "lgbm_search = RandomizedSearchCV(\n",
        "    estimator=lgbm_clf,\n",
        "    param_distributions=lgbm_param_grid,\n",
        "    n_iter=20,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "lgbm_search.fit(X_train, y_train)\n",
        "\n",
        "best_lgbm = lgbm_search.best_estimator_\n",
        "print(\"âœ… LightGBM Classifier tuned â€” Best params:\", lgbm_search.best_params_)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# STEP 6 â€” STACKED CLASSIFIER\n",
        "# ==========================================\n",
        "\n",
        "stacked_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', best_xgb),\n",
        "        ('lgbm', best_lgbm)\n",
        "    ],\n",
        "    final_estimator=XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "stacked_clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"âœ… Stacked Classifier trained ðŸš€\")\n",
        "\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# STEP 7 â€” EVALUATION\n",
        "# ==========================================\n",
        "\n",
        "# Predict\n",
        "y_pred = stacked_clf.predict(X_test)\n",
        "\n",
        "# Metrics\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"ðŸŽ¯ STACKED CLASSIFIER PERFORMANCE:\")\n",
        "print(\"Accuracy:\", acc)\n",
        "print(report)\n",
        "\n",
        "# Confusion Matrix Plot\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix â€” Stacked Classifier')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# STEP 8 â€” SHAP ANALYSIS (Explainability)\n",
        "# ==========================================\n",
        "\n",
        "# Use SHAP on best base model (XGBoost is common)\n",
        "explainer = shap.Explainer(best_xgb, X_train)\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# Summary Plot\n",
        "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
        "\n",
        "# Full SHAP plot (optional â€” big)\n",
        "# shap.summary_plot(shap_values, X_test)\n",
        "\n",
        "# ==========================================\n",
        "# STEP 9 â€” FINAL CELL â†’ MANUAL INPUT FOR MONTHLY PREDICTION\n",
        "# ==========================================\n",
        "\n",
        "# ðŸ‘‰ You enter latest macro values â†’ model predicts HIGHER or LOWER ðŸš€\n",
        "\n",
        "# Example â€” replace with YOUR latest data:\n",
        "manual_input = pd.DataFrame([{\n",
        "    'Delta_Prev_Forecast': -5000,\n",
        "    'Momentum': 3000,\n",
        "    'Month': 6,\n",
        "    'Quarter': 2,\n",
        "    'Lag1_NFP': 250000,\n",
        "    'Lag2_NFP': 240000,\n",
        "    'Rolling_Vol_3M': 18000,\n",
        "    'Rolling_Vol_6M': 22000,\n",
        "    'ADP_Pct_Change': 0.05,\n",
        "    'Jobless_4Wk_Avg': 215000,\n",
        "    'ISM_PMI_1M_Change': 1.2,\n",
        "    'ISM_PMI_3M_Avg': 53.5,\n",
        "    'Yield_10Y_Change': 0.10\n",
        "}])\n",
        "\n",
        "# Predict\n",
        "manual_pred = stacked_clf.predict(manual_input)\n",
        "manual_prob = stacked_clf.predict_proba(manual_input)\n",
        "\n",
        "# Output\n",
        "print(\"ðŸš€ FINAL MONTHLY PREDICTION:\")\n",
        "print(\"Predicted Class (1 = Higher, 0 = Lower):\", manual_pred[0])\n",
        "print(\"Probability (Lower / Higher):\", manual_prob[0])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "MbMcw0kcQ8vI",
        "outputId": "7703754f-814a-401f-fd07-8c552956b05a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Libraries loaded ðŸš€\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'YOUR_FEATURE_DATASET.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7f311fcc48e1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Load your dataset:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'YOUR_FEATURE_DATASET.csv'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace with your feature dataset path!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# Target: Classification â†’ NFP Higher (1) / Lower (0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'YOUR_FEATURE_DATASET.csv'"
          ]
        }
      ]
    }
  ]
}